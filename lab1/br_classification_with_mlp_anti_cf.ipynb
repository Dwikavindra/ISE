{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Text and feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Evaluation and tuning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_curve, auc)\n",
    "\n",
    "# Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Text cleaning & stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from br_classification import remove_html,remove_emoji,remove_stopwords,clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_stop_words_list = stopwords.words('english')\n",
    "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
    "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
    "\n",
    "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
    "pd_all['Title+Body'] = pd_all.apply(\n",
    "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
    "pd_tplusb = pd_all.rename(columns={\n",
    "    \"Unnamed: 0\": \"id\",\n",
    "    \"class\": \"sentiment\",\n",
    "    \"Title+Body\": \"text\"\n",
    "})\n",
    "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 4. Configure parameters & Start training ##########\n",
    "\n",
    "# ========== Key Configurations ==========\n",
    "\n",
    "# 1) Data file to read\n",
    "datafile = 'Title+Body.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "out_csv_name = f'../{project}_NB.csv'\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "\n",
    "# ========== Hyperparameter grid ==========\n",
    "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}\n",
    "\n",
    "# Lists to store metrics across repeated runs\n",
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticf_model import AntiCFTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[text_col], data['sentiment'], train_size=0.8, random_state=42)\n",
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.toarray(), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AntiCFTextClassifier(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6411, Accuracy: 62.56%\n",
      "Epoch [2/10], Loss: 0.4115, Accuracy: 85.69%\n",
      "Epoch [3/10], Loss: 0.1650, Accuracy: 95.01%\n",
      "Epoch [4/10], Loss: 0.1355, Accuracy: 95.51%\n",
      "Epoch [5/10], Loss: 0.1545, Accuracy: 93.01%\n",
      "Epoch [6/10], Loss: 0.1443, Accuracy: 92.68%\n",
      "Epoch [7/10], Loss: 0.1191, Accuracy: 94.84%\n",
      "Epoch [8/10], Loss: 0.0912, Accuracy: 97.00%\n",
      "Epoch [9/10], Loss: 0.0673, Accuracy: 98.00%\n",
      "Epoch [10/10], Loss: 0.0499, Accuracy: 99.17%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    _,outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Compute loss\n",
    "    \n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "    correct = (predictions == y_train).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_train.size(0)  # Compute accuracy percentage\n",
    "    \n",
    "    # Backward pass & optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1: 8609.27%\n",
      "Accuracy 2: 8609.27%\n",
      "Accuracy 3: 8609.27%\n",
      "Accuracy 4: 8609.27%\n",
      "Accuracy 5: 8609.27%\n",
      "Accuracy 6: 8609.27%\n",
      "Accuracy 7: 8609.27%\n",
      "Accuracy 8: 8609.27%\n",
      "Accuracy 9: 8609.27%\n",
      "Accuracy 10: 8609.27%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with torch.no_grad():\n",
    "        _,outputs = model(X_test)\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = (predictions == y_test).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_test.size(0) * 100  # Compute accuracy percentage\n",
    "    print(f\"Accuracy {epoch+1}: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Different Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'caffe'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
    "\n",
    "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
    "pd_all['Title+Body'] = pd_all.apply(\n",
    "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
    "pd_tplusb = pd_all.rename(columns={\n",
    "    \"Unnamed: 0\": \"id\",\n",
    "    \"class\": \"sentiment\",\n",
    "    \"Title+Body\": \"text\"\n",
    "})\n",
    "pd_tplusb.to_csv('Title+BodyKeras.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
    "\n",
    "########## 4. Configure parameters & Start training ##########\n",
    "\n",
    "# ========== Key Configurations ==========\n",
    "\n",
    "# 1) Data file to read\n",
    "datafile = 'Title+BodyKeras.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "out_csv_name = f'../{project}_NB.csv'\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "\n",
    "# ========== Hyperparameter grid ==========\n",
    "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}\n",
    "\n",
    "# Lists to store metrics across repeated runs\n",
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data[text_col], data['sentiment'], train_size=0.8, random_state=42)\n",
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_new = tfidf.fit_transform(data[text_col])\n",
    "X_new = torch.tensor(X_new.toarray(), dtype=torch.float32)\n",
    "y_new= torch.tensor(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1: 8846.15%\n",
      "Accuracy 2: 8846.15%\n",
      "Accuracy 3: 8846.15%\n",
      "Accuracy 4: 8846.15%\n",
      "Accuracy 5: 8846.15%\n",
      "Accuracy 6: 8846.15%\n",
      "Accuracy 7: 8846.15%\n",
      "Accuracy 8: 8846.15%\n",
      "Accuracy 9: 8846.15%\n",
      "Accuracy 10: 8846.15%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with torch.no_grad():\n",
    "        _,outputs = model(X_new)\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = (predictions == y_new).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_new.size(0) * 100  # Compute accuracy percentage\n",
    "    print(f\"Accuracy {epoch+1}: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Tent on TItle Body Caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tent\n",
    "import TextDataset\n",
    "import importlib\n",
    "# import batch_data_loader\n",
    "importlib.reload(TextDataset)  \n",
    "importlib.reload(tent)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "9    unable to reproduce accuracy of bvlc-alexnet. ...          1\n",
      "267  osx: abs not defined absval_layer. When compil...          0\n",
      "143  cafe_intsall.caffe 36 error. I am trying caffe...          0\n",
      "212   undefined reference to `lzma_index_end@XZ_5.0...          0\n",
      "227  Dimension mismatch training with my own model ...          0\n"
     ]
    }
   ],
   "source": [
    "dataset=TextDataset.TextDatasetTFIDF(\"datasets/caffe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "        if \"adapter\" not in name:\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anticf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AntiCF Training: 100%|██████████| 5/5 [00:00<00:00, 26.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Accuracy: 0.8636\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 0.8636363636363636)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result=anticf.AntiCF(model,dataset)\n",
    "print(result)\n",
    "tent.reset(model,model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
