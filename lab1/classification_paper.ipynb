{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Text and feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Evaluation and tuning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_curve, auc)\n",
    "\n",
    "# Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Text cleaning & stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from br_classification import remove_html,remove_emoji,remove_stopwords,clean_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'Title+Body.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col]= data[text_col].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " 'doc',\n",
       " 'global',\n",
       " 'op',\n",
       " 'carried',\n",
       " 'source',\n",
       " '.',\n",
       " 'bug',\n",
       " 'missing',\n",
       " 'many',\n",
       " 'global',\n",
       " 'operators',\n",
       " '(',\n",
       " ',',\n",
       " ',',\n",
       " ')',\n",
       " '.',\n",
       " 'reproduce',\n",
       " 'expected',\n",
       " 'behavior',\n",
       " 'global',\n",
       " 'ops',\n",
       " 'contain',\n",
       " 'valid',\n",
       " 's.',\n",
       " 'environment',\n",
       " 'pytorch',\n",
       " 'version',\n",
       " '1.0.1.post2',\n",
       " 'debug',\n",
       " 'build',\n",
       " 'cuda',\n",
       " 'used',\n",
       " 'build',\n",
       " 'pytorch',\n",
       " '9.0.176',\n",
       " 'os',\n",
       " 'ubuntu',\n",
       " '16.04.5',\n",
       " 'lts',\n",
       " 'gcc',\n",
       " 'version',\n",
       " '(',\n",
       " 'ubuntu',\n",
       " '5.4.0',\n",
       " '6ubuntu1',\n",
       " '16.04.11',\n",
       " ')',\n",
       " '5.4.0',\n",
       " '20160609',\n",
       " 'cmake',\n",
       " 'version',\n",
       " 'version',\n",
       " '3.9.4',\n",
       " 'python',\n",
       " 'version',\n",
       " '3.6',\n",
       " 'cuda',\n",
       " 'available',\n",
       " 'yes',\n",
       " 'cuda',\n",
       " 'runtime',\n",
       " 'version',\n",
       " 'could',\n",
       " 'collect',\n",
       " 'gpu',\n",
       " 'models',\n",
       " 'configuration',\n",
       " 'gpu',\n",
       " '0',\n",
       " 'geforce',\n",
       " 'gtx',\n",
       " 'titan',\n",
       " 'black',\n",
       " 'gpu',\n",
       " '1',\n",
       " 'geforce',\n",
       " 'gtx',\n",
       " 'titan',\n",
       " 'black',\n",
       " 'gpu',\n",
       " '2',\n",
       " 'geforce',\n",
       " 'gtx',\n",
       " 'titan',\n",
       " 'black',\n",
       " 'gpu',\n",
       " '3',\n",
       " 'geforce',\n",
       " 'gtx',\n",
       " 'titan',\n",
       " 'black',\n",
       " 'nvidia',\n",
       " 'driver',\n",
       " 'version',\n",
       " '390.30',\n",
       " 'cudnn',\n",
       " 'version',\n",
       " 'usr',\n",
       " 'lib',\n",
       " 'x86',\n",
       " '64',\n",
       " 'linux',\n",
       " 'gnu',\n",
       " 'libcudnn.so.7.1.1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[text_col][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punkt(tokens):\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    return [word for word in tokens if not all(char in punctuation_set for char in word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[text_col]= data[text_col].apply(remove_punkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The paper mentions the following:\n",
    " After tokenization and stop word removal,\n",
    "the words were stemmed into their lemma (root). For example,\n",
    "the words: ‘execution’, ‘executed’, ‘executable’ were stemmed\n",
    "into their lemma ‘execute’.\n",
    "In Table 2, we have represented the output generated by\n",
    "our pre-processing steps for two example sentences. In the\n",
    "first row of the table, words such as ‘after’, ‘to’, and ‘the’\n",
    "have been removed. Subsequently, the word ‘added’ has been\n",
    "stemmed into ‘add’ (the root word). Similarly, the second row\n",
    "has been pre-processed into ‘Prime face be found’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[text_col]= data[text_col].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply WMD (Word Mover Distance) approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document(X_test_withYPred,model):\n",
    "    min_distance = float('inf')\n",
    "    best_class=None \n",
    "    X_pred_result=X_test_withYPred.copy()\n",
    "    for id_pred, text_pred,_ in X_pred_result.itertuples(index=False,name=None):\n",
    "        for id_real, text_real,sentiment_real in X_test_withYPred.itertuples(index=False,name=None):\n",
    "            if id_pred == id_real: #avoid comparing to the same docs\n",
    "                continue\n",
    "            distance = model.wmdistance(text_pred,text_real)\n",
    "        \n",
    "            if distance < min_distance:\n",
    "            #get the smallest distance and change the label\n",
    "                min_distance = distance\n",
    "                best_class = sentiment_real\n",
    "        X_pred_result.loc[X_pred_result[\"id\"] == id_pred, \"sentiment\"] = best_class #save the result\n",
    "        best_class=None #reset the classes for the next for loop\n",
    "        min_distance=float('inf')\n",
    "\n",
    "    return X_pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>688</td>\n",
       "      <td>[python, doc, global, op, carried, source, bug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>616</td>\n",
       "      <td>[dataloader, segmentation, fault, using, mpi, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>322</td>\n",
       "      <td>[torch.from, pil, request, feature, simple, me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>[torch.halftensor, object, attribute, mean, re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>647</td>\n",
       "      <td>[support, dilation, conv1d, conv3d]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>225</td>\n",
       "      <td>[discussion, recommend, different, file, exten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>712</td>\n",
       "      <td>[feature, request, add, support, selu, activat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>481</td>\n",
       "      <td>[jit, error, reporting, imported, module, high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>348</td>\n",
       "      <td>[libtorch, segmentation, fault, rhel, 7, easy,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>448</td>\n",
       "      <td>[tracking, issue, rpc, test, flaky, cc, ezyang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  sentiment\n",
       "0    688  [python, doc, global, op, carried, source, bug...          0\n",
       "1    616  [dataloader, segmentation, fault, using, mpi, ...          0\n",
       "2    322  [torch.from, pil, request, feature, simple, me...          0\n",
       "3    241  [torch.halftensor, object, attribute, mean, re...          0\n",
       "4    647                [support, dilation, conv1d, conv3d]          0\n",
       "..   ...                                                ...        ...\n",
       "747  225  [discussion, recommend, different, file, exten...          0\n",
       "748  712  [feature, request, add, support, selu, activat...          0\n",
       "749  481  [jit, error, reporting, imported, module, high...          0\n",
       "750  348  [libtorch, segmentation, fault, rhel, 7, easy,...          0\n",
       "751  448  [tracking, issue, rpc, test, flaky, cc, ezyang...          0\n",
       "\n",
       "[752 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_with_Y_Pred=pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\"id\",text_col,\"sentiment\"],  \n",
    " \n",
    ")\n",
    "X_test_with_Y_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_result=classify_document(X_test_with_Y_Pred,model)\n",
    "X_result.to_csv(\"word2mov.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_result=pd.read_csv('word2mov.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(X_test_with_Y_Pred['sentiment'], X_result['sentiment'])\n",
    "prec = precision_score(X_test_with_Y_Pred['sentiment'], X_result['sentiment'], average='macro')\n",
    "\n",
    "\n",
    "    # Recall (macro)\n",
    "rec = recall_score(X_test_with_Y_Pred['sentiment'], X_result['sentiment'], average='macro')\n",
    "\n",
    "\n",
    "    # F1 Score (macro)\n",
    "f1 = f1_score(X_test_with_Y_Pred['sentiment'], X_result['sentiment'], average='macro')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(X_test_with_Y_Pred['sentiment'], X_result['sentiment'], pos_label=1)\n",
    "auc_val = auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.8684\n",
      "Precision:     0.7005\n",
      "Recall:        0.6950\n",
      "F1 score:      0.6977\n",
      "AUC:           0.6950\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy:      {acc:.4f}\")\n",
    "print(f\"Precision:     {prec:.4f}\")\n",
    "print(f\"Recall:        {rec:.4f}\")\n",
    "print(f\"F1 score:      {f1:.4f}\")\n",
    "print(f\"AUC:           {auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the model with Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## according to the paper we need to remove wrongful prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_result = X_result[X_test_with_Y_Pred['sentiment'] == X_result[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>616</td>\n",
       "      <td>['dataloader', 'segmentation', 'fault', 'using...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>322</td>\n",
       "      <td>['torch.from', 'pil', 'request', 'feature', 's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>['torch.halftensor', 'object', 'attribute', 'm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>647</td>\n",
       "      <td>['support', 'dilation', 'conv1d', 'conv3d']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>701</td>\n",
       "      <td>['better', 'error', 'message', 'compiling', 'c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>225</td>\n",
       "      <td>['discussion', 'recommend', 'different', 'file...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>712</td>\n",
       "      <td>['feature', 'request', 'add', 'support', 'selu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>481</td>\n",
       "      <td>['jit', 'error', 'reporting', 'imported', 'mod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>348</td>\n",
       "      <td>['libtorch', 'segmentation', 'fault', 'rhel', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>448</td>\n",
       "      <td>['tracking', 'issue', 'rpc', 'test', 'flaky', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  sentiment\n",
       "1    616  ['dataloader', 'segmentation', 'fault', 'using...          0\n",
       "2    322  ['torch.from', 'pil', 'request', 'feature', 's...          0\n",
       "3    241  ['torch.halftensor', 'object', 'attribute', 'm...          0\n",
       "4    647        ['support', 'dilation', 'conv1d', 'conv3d']          0\n",
       "5    701  ['better', 'error', 'message', 'compiling', 'c...          0\n",
       "..   ...                                                ...        ...\n",
       "747  225  ['discussion', 'recommend', 'different', 'file...          0\n",
       "748  712  ['feature', 'request', 'add', 'support', 'selu...          0\n",
       "749  481  ['jit', 'error', 'reporting', 'imported', 'mod...          0\n",
       "750  348  ['libtorch', 'segmentation', 'fault', 'rhel', ...          0\n",
       "751  448  ['tracking', 'issue', 'rpc', 'test', 'flaky', ...          0\n",
       "\n",
       "[653 rows x 3 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impelement Naive Bayes after WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(list):\n",
    "    res = ' '.join(list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Naive_Bayes=X_pred_result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the strings back for tf idf\n",
    "\n",
    "# \" \".join(str(x) for x in X_Naive_Bayes[text_col])\n",
    "X_Naive_Bayes[text_col]=X_Naive_Bayes[text_col].apply(ast.literal_eval)\n",
    "X_Naive_Bayes[text_col]=X_Naive_Bayes[text_col].apply(listToString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      dataloader segmentation fault using mpi backen...\n",
       "2      torch.from pil request feature simple method t...\n",
       "3      torch.halftensor object attribute mean result ...\n",
       "4                         support dilation conv1d conv3d\n",
       "5      better error message compiling cudnn v5 which ...\n",
       "                             ...                        \n",
       "747    discussion recommend different file extension ...\n",
       "748    feature request add support selu activation ca...\n",
       "749    jit error reporting imported module highlight ...\n",
       "750    libtorch segmentation fault rhel 7 easy reprod...\n",
       "751    tracking issue rpc test flaky cc ezyang gchana...\n",
       "Name: text, Length: 653, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Naive_Bayes[text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9351145038167938\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(X_Naive_Bayes[\"text\"])  # Convert text into numerical vectors\n",
    "y =X_Naive_Bayes[\"sentiment\"]  # Labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# --- 1️⃣ Naive Bayes Classifier ---\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_train_nb = nb_model.predict(X_train)  # Predictions for training set\n",
    "y_pred_nb = nb_model.predict(X_test)   \n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "# the algo on the paper says we should put the result of x_test and y_test again to the next model \n",
    "#as such we would input X_test into train_test_split\n",
    "# # --- 2️⃣ KNN Classifier (Using NB Predictions as Input) ---\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "# knn_model.fit(X_test, y_pred_nb)  # Training KNN on Naive Bayes predictions\n",
    "# y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# # --- 3️⃣ Random Forest Classifier (Using KNN Predictions as Input) ---\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_pred_knn)  # Training RF on KNN predictions\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# # Final Evaluation\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9312977099236641"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)  # Training KNN on Naive Bayes predictions\n",
    "y_train_knn = knn_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_train_knn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9351145038167938"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train_knn)  # Ensure training set matches\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
