{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from '/Volumes/Dwika/ISE/ISE-solution/lab1/helper.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import TextDataset\n",
    "import importlib\n",
    "importlib.reload(TextDataset)\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['caffe', 'incubator-mxnet', 'keras', 'pytorch', 'tensorflow']\n",
    "RESULTS = []\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "VAL_RATIO = 0.2\n",
    "INPUT_DIM = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "val_loaders=[]\n",
    "train_loaders=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train and validation differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "9    unable to reproduce accuracy of bvlc-alexnet. ...          1\n",
      "267  osx: abs not defined absval_layer. When compil...          0\n",
      "143  cafe_intsall.caffe 36 error. I am trying caffe...          0\n",
      "212   undefined reference to `lzma_index_end@XZ_5.0...          0\n",
      "227  Dimension mismatch training with my own model ...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "304  Do I need to change grad_req when sharing weig...          0\n",
      "500  docs for gluon.data.* are missing. Probably re...          0\n",
      "441  Does gluon's dnn support data format of libsvm...          0\n",
      "153  Building with OpenCV causes link errors. ## De...          0\n",
      "502  #12285 Breaks NDArrayIter For 3D Arrays. ## De...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "361  How to represent 28x1x1 output dense layer?. !...          0\n",
      "158  Accessing internal states. Hey guys, cool proj...          0\n",
      "477  docs correction: https://keras.io/activations/...          0\n",
      "517  SimpleRNN - Wrong number of dimensions, expect...          0\n",
      "275  merge layer documentation and architecture  . ...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "584  undefined symbol: cudnnSetConvolutionGroupCoun...          0\n",
      "591  Unable to build PyTorch from source without Nu...          0\n",
      "486  [docs] Make clear the format of torch.eig eige...          0\n",
      "77   `index_select` on flat tensor faster than inte...          1\n",
      "212  test_det_logdet_slogdet_batched (in test_cuda....          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "941  Graph_def is invalid at node u'ExpandDims': In...          0\n",
      "297  stream_executor/platform/mutex.h doesn't compi...          0\n",
      "271  [Perfomance]Dilated/Atrous Conv implementation...          1\n",
      "774  Tensorflow 2.0 keras load_model does not resto...          0\n",
      "420  Multiprocessing for input pipeline . I have as...          0\n"
     ]
    }
   ],
   "source": [
    "for names in DATASETS:\n",
    "    dataset = TextDataset.TextDatasetTFIDF(f'datasets/{names}.csv')\n",
    "    val_size = int(len(dataset) * VAL_RATIO)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    val_loaders.append({\n",
    "         f'base-dataset-{names}':val_loader\n",
    "    })\n",
    "    train_loaders.append({\n",
    "        f'base-dataset-{names}':train_loader\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loader_by_key(loader_list, key):\n",
    "    for loader_dict in loader_list:\n",
    "        if key in loader_dict:\n",
    "            return loader_dict[key]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for names in DATASETS:\n",
    "    train_loader=find_loader_by_key(train_loaders,f'base-dataset-{names}')\n",
    "    model= helper.train_model(train_loader)\n",
    "    models.append({\n",
    "        f'base-dataset-{names}':model\n",
    "    })\n",
    "    val_loaders.append({\n",
    "         f'base-dataset-{names}':val_loader\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to models/base-dataset-caffe.pt\n",
      "Saved model to models/base-dataset-incubator-mxnet.pt\n",
      "Saved model to models/base-dataset-keras.pt\n",
      "Saved model to models/base-dataset-pytorch.pt\n",
      "Saved model to models/base-dataset-tensorflow.pt\n"
     ]
    }
   ],
   "source": [
    "for model_dict in models:\n",
    "    for name, model in model_dict.items():\n",
    "        path = os.path.join(\"models\", f\"{name}.pt\")\n",
    "        torch.save(model.state_dict(), path) # save only the weigths\n",
    "        print(f\"Saved model to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can skip the above if all the models has been saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the models back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_mlp import MLPWithLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if models is None:\n",
    "#     models=[]\n",
    "# if len(models)==0:\n",
    "#     model_folder = \"models\"\n",
    "#     # Loop through each .pt file in the folder\n",
    "#     for filename in sorted(os.listdir(model_folder)):\n",
    "#         if filename.endswith(\".pt\"):\n",
    "#             model_name = filename.replace(\".pt\", \"\")  # e.g., 'base-dataset-caffe'\n",
    "#             model_path = os.path.join(model_folder, filename)\n",
    "#             # Re-initialize a fresh model instance\n",
    "#             model = MLPWithLayerNorm(input_dim=1000)\n",
    "#             model.load_state_dict(torch.load(model_path))\n",
    "#             models.append({\n",
    "#                 model_name:model\n",
    "#             })\n",
    "#             print(f\"Loaded: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the models on inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_evaluation=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    for model_dict in models:\n",
    "        for name, model in model_dict.items():\n",
    "            # No need to reset the weights just yet cause the model is not learning with Tent\n",
    "            val_loader= find_loader_by_key(val_loaders,name)  \n",
    "            result=helper.evaluate_model(model,val_loader,name,name,i)\n",
    "            results_evaluation.append(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all base model results to result/base_models.csv\n"
     ]
    }
   ],
   "source": [
    "# Flatten the nested results into a list of dicts\n",
    "os.makedirs(\"result\", exist_ok=True)\n",
    "columns = [\"iteration\", \"name\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "with open(\"result/base_models.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Loop through all results and write each row\n",
    "    for results in results_evaluation:\n",
    "        row = {col: results.get(col, None) for col in columns}\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "print(\"Saved all base model results to result/base_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data to dataframe to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_models = pd.read_csv(\"result/base_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base-dataset-caffe->base-dataset-caffe',\n",
       " 'base-dataset-incubator-mxnet->base-dataset-incubator-mxnet',\n",
       " 'base-dataset-keras->base-dataset-keras',\n",
       " 'base-dataset-pytorch->base-dataset-pytorch',\n",
       " 'base-dataset-tensorflow->base-dataset-tensorflow']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = df_base_models[\"name\"].unique().tolist()\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    subset = df_base_models[df_base_models[\"name\"] == name]\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    stats = {\n",
    "        \"name\": name,\n",
    "        \"mean_accuracy\": subset[\"accuracy\"].mean(),\n",
    "        \"std_accuracy\": subset[\"accuracy\"].std(),\n",
    "        \"mean_precision\": subset[\"precision\"].mean(),\n",
    "        \"std_precision\": subset[\"precision\"].std(),\n",
    "        \"mean_recall\": subset[\"recall\"].mean(),\n",
    "        \"std_recall\": subset[\"recall\"].std(),\n",
    "        \"mean_f1\": subset[\"f1\"].mean(),\n",
    "        \"std_f1\": subset[\"f1\"].std()\n",
    "    }\n",
    "\n",
    "    summary_stats.append(stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>mean_precision</th>\n",
       "      <th>std_precision</th>\n",
       "      <th>mean_recall</th>\n",
       "      <th>std_recall</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>std_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base-dataset-caffe-&gt;base-dataset-caffe</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>5.607473e-16</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.682242e-16</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.962616e-16</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.411210e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base-dataset-incubator-mxnet-&gt;base-dataset-inc...</td>\n",
       "      <td>0.902913</td>\n",
       "      <td>5.607473e-16</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.364484e-16</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>3.925231e-16</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>3.364484e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base-dataset-keras-&gt;base-dataset-keras</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1.121495e-16</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>6.728968e-16</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>2.242989e-16</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>5.607473e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base-dataset-pytorch-&gt;base-dataset-pytorch</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>4.485978e-16</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>4.485978e-16</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>5.607473e-17</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>5.046726e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>base-dataset-tensorflow-&gt;base-dataset-tensorflow</td>\n",
       "      <td>0.889262</td>\n",
       "      <td>5.607473e-16</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>2.242989e-16</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.364484e-16</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>1.121495e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  mean_accuracy  \\\n",
       "0             base-dataset-caffe->base-dataset-caffe       0.859649   \n",
       "1  base-dataset-incubator-mxnet->base-dataset-inc...       0.902913   \n",
       "2             base-dataset-keras->base-dataset-keras       0.842105   \n",
       "3         base-dataset-pytorch->base-dataset-pytorch       0.880000   \n",
       "4   base-dataset-tensorflow->base-dataset-tensorflow       0.889262   \n",
       "\n",
       "   std_accuracy  mean_precision  std_precision  mean_recall    std_recall  \\\n",
       "0  5.607473e-16        0.333333   1.682242e-16     0.142857  1.962616e-16   \n",
       "1  5.607473e-16        0.666667   3.364484e-16     0.461538  3.925231e-16   \n",
       "2  1.121495e-16        0.681818   6.728968e-16     0.517241  2.242989e-16   \n",
       "3  4.485978e-16        0.615385   4.485978e-16     0.380952  5.607473e-17   \n",
       "4  5.607473e-16        0.680000   2.242989e-16     0.666667  3.364484e-16   \n",
       "\n",
       "    mean_f1        std_f1  \n",
       "0  0.200000  8.411210e-17  \n",
       "1  0.545455  3.364484e-16  \n",
       "2  0.588235  5.607473e-16  \n",
       "3  0.470588  5.046726e-16  \n",
       "4  0.673267  1.121495e-16  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Tent to improve upon the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['caffe', 'incubator-mxnet', 'keras', 'pytorch', 'tensorflow']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models is None:\n",
    "    models=[]\n",
    "if len(models)==0:\n",
    "    model_folder = \"models\"\n",
    "    # Loop through each .pt file in the folder\n",
    "    for filename in sorted(os.listdir(model_folder)):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            model_name = filename.replace(\".pt\", \"\")  # e.g., 'base-dataset-caffe'\n",
    "            model_path = os.path.join(model_folder, filename)\n",
    "            # Re-initialize a fresh model instance\n",
    "            model = MLPWithLayerNorm(input_dim=1000)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            models.append({\n",
    "                model_name:model\n",
    "            })\n",
    "            print(f\"Loaded: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tent Each model and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tent' from '/Volumes/Dwika/ISE/ISE-solution/lab1/tent.py'>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tent\n",
    "import importlib\n",
    "importlib.reload(tent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on different dataset for 50 times on each dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: base-dataset-caffe\n",
      "Loaded: base-dataset-incubator-mxnet\n",
      "Loaded: base-dataset-keras\n",
      "Loaded: base-dataset-pytorch\n",
      "Loaded: base-dataset-tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/f7z3n2sd0ygb8_hfzm0lbzb40000gn/T/ipykernel_11501/1498546071.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "if models_tented is None:\n",
    "    models_tented=[]\n",
    "if len(models_tented)==0:\n",
    "    model_folder = \"models\"\n",
    "    # Loop through each .pt file in the folder\n",
    "    for filename in sorted(os.listdir(model_folder)):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            model_name = filename.replace(\".pt\", \"\")  # e.g., 'base-dataset-caffe'\n",
    "            model_path = os.path.join(model_folder, filename)\n",
    "            # Re-initialize a fresh model instance\n",
    "            model = MLPWithLayerNorm(input_dim=1000)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model= tent.configureTent(model)\n",
    "            models_tented.append({\n",
    "                model_name:model\n",
    "            })\n",
    "            print(f\"Loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer 50 times on one model to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "9    unable to reproduce accuracy of bvlc-alexnet. ...          1\n",
      "267  osx: abs not defined absval_layer. When compil...          0\n",
      "143  cafe_intsall.caffe 36 error. I am trying caffe...          0\n",
      "212   undefined reference to `lzma_index_end@XZ_5.0...          0\n",
      "227  Dimension mismatch training with my own model ...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "304  Do I need to change grad_req when sharing weig...          0\n",
      "500  docs for gluon.data.* are missing. Probably re...          0\n",
      "441  Does gluon's dnn support data format of libsvm...          0\n",
      "153  Building with OpenCV causes link errors. ## De...          0\n",
      "502  #12285 Breaks NDArrayIter For 3D Arrays. ## De...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "361  How to represent 28x1x1 output dense layer?. !...          0\n",
      "158  Accessing internal states. Hey guys, cool proj...          0\n",
      "477  docs correction: https://keras.io/activations/...          0\n",
      "517  SimpleRNN - Wrong number of dimensions, expect...          0\n",
      "275  merge layer documentation and architecture  . ...          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "584  undefined symbol: cudnnSetConvolutionGroupCoun...          0\n",
      "591  Unable to build PyTorch from source without Nu...          0\n",
      "486  [docs] Make clear the format of torch.eig eige...          0\n",
      "77   `index_select` on flat tensor faster than inte...          1\n",
      "212  test_det_logdet_slogdet_batched (in test_cuda....          0\n",
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "941  Graph_def is invalid at node u'ExpandDims': In...          0\n",
      "297  stream_executor/platform/mutex.h doesn't compi...          0\n",
      "271  [Perfomance]Dilated/Atrous Conv implementation...          1\n",
      "774  Tensorflow 2.0 keras load_model does not resto...          0\n",
      "420  Multiprocessing for input pipeline . I have as...          0\n"
     ]
    }
   ],
   "source": [
    "for names in DATASETS:\n",
    "    dataset = TextDataset.TextDatasetTFIDF(f'datasets/{names}.csv')\n",
    "    data_loader=DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    data_loaders.append({\n",
    "        f'base-dataset-{names}':data_loader\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base-dataset-caffe': <torch.utils.data.dataloader.DataLoader at 0x30e50e4d0>},\n",
       " {'base-dataset-incubator-mxnet': <torch.utils.data.dataloader.DataLoader at 0x310840750>},\n",
       " {'base-dataset-keras': <torch.utils.data.dataloader.DataLoader at 0x3279dbc50>},\n",
       " {'base-dataset-pytorch': <torch.utils.data.dataloader.DataLoader at 0x32c9f0790>},\n",
       " {'base-dataset-tensorflow': <torch.utils.data.dataloader.DataLoader at 0x30e281050>}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tent_all_results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame:\n",
      "                                                   text  sentiment\n",
      "941  Graph_def is invalid at node u'ExpandDims': In...          0\n",
      "297  stream_executor/platform/mutex.h doesn't compi...          0\n",
      "271  [Perfomance]Dilated/Atrous Conv implementation...          1\n",
      "774  Tensorflow 2.0 keras load_model does not resto...          0\n",
      "420  Multiprocessing for input pipeline . I have as...          0\n"
     ]
    }
   ],
   "source": [
    "dataset_tent=TextDataset.TextDatasetTFIDF(\"datasets/tensorflow.csv\")\n",
    "dataloader_tent= DataLoader(dataset_tent, batch_size=512, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'base-dataset-caffe': MLPWithLayerNorm(\n",
       "    (fc1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )},\n",
       " {'base-dataset-incubator-mxnet': MLPWithLayerNorm(\n",
       "    (fc1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )},\n",
       " {'base-dataset-keras': MLPWithLayerNorm(\n",
       "    (fc1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )},\n",
       " {'base-dataset-pytorch': MLPWithLayerNorm(\n",
       "    (fc1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )},\n",
       " {'base-dataset-tensorflow': MLPWithLayerNorm(\n",
       "    (fc1): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )}]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tent_model=tent.configureTent(models[0][\"base-dataset-caffe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 3/3 [00:01<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 81.28%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5000\n",
      "{'iteration': 1, 'name': 'caffe->tensorflow', 'accuracy': 0.812751677852349, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "result=tent.Tent(tent_model,dataloader_tent,\"caffe\",\"tensorflow\",1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.21%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.4941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 122.63it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 81.22it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 50.99it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 61.58it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 50.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 28.35it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 69.61it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 63.15it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent: 100%|██████████| 9/9 [00:00<00:00, 44.34it/s]\n",
      "/Users/dwika/miniconda3/envs/fyp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test-Time Adaptation (Tent) Metrics:\n",
      "  Accuracy : 87.40%\n",
      "  F1 Score : 0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall   : 0.0000\n",
      "  ROC AUC  : 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapting with Tent:  11%|█         | 1/9 [00:00<00:02,  2.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_dataset \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_name:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Not the right dataset to test on\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m tent_all_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/Volumes/Dwika/ISE/ISE-solution/lab1/tent.py:42\u001b[0m, in \u001b[0;36mTent\u001b[0;34m(model, eval_loader, origin_dataset_name, target_dataset_name, iteration)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m entropy(outputs)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 42\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fyp/lib/python3.11/site-packages/torch/optim/adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_dict in models_tented:\n",
    "    for model_name, model in model_dict.items():\n",
    "        # Extract the source dataset from the model name\n",
    "        for dataset_name in DATASETS:\n",
    "            if dataset_name in model_name:\n",
    "                model_dataset = dataset_name\n",
    "                break  # We found the matching dataset used for training\n",
    "\n",
    "        for target_dataset in DATASETS:\n",
    "            if target_dataset == model_dataset:\n",
    "                continue  # Skip if same dataset as model's training data\n",
    "\n",
    "            for i in range(50):\n",
    "                for loader_dict in data_loaders:\n",
    "                    for loader_name, data_loader in loader_dict.items():\n",
    "                        if target_dataset not in loader_name:\n",
    "                            continue  # Not the right dataset to test on\n",
    "                        \n",
    "                        result = tent.Tent(model, data_loader, model_name, loader_name, i)\n",
    "                        tent_all_results.append(result)\n",
    "                        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
