{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Text and feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Evaluation and tuning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_curve, auc)\n",
    "\n",
    "# Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Text cleaning & stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from br_classification import remove_html,remove_emoji,remove_stopwords,clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_stop_words_list = stopwords.words('english')\n",
    "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
    "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
    "\n",
    "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
    "pd_all['Title+Body'] = pd_all.apply(\n",
    "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
    "pd_tplusb = pd_all.rename(columns={\n",
    "    \"Unnamed: 0\": \"id\",\n",
    "    \"class\": \"sentiment\",\n",
    "    \"Title+Body\": \"text\"\n",
    "})\n",
    "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 4. Configure parameters & Start training ##########\n",
    "\n",
    "# ========== Key Configurations ==========\n",
    "\n",
    "# 1) Data file to read\n",
    "datafile = 'Title+Body.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "out_csv_name = f'../{project}_NB.csv'\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "\n",
    "# ========== Hyperparameter grid ==========\n",
    "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}\n",
    "\n",
    "# Lists to store metrics across repeated runs\n",
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_mlp import MLPWithLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[text_col], data['sentiment'], train_size=0.8, random_state=42)\n",
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.toarray(), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPWithLayerNorm(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6926, Accuracy: 51.58%\n",
      "Epoch [2/10], Loss: 0.4307, Accuracy: 87.69%\n",
      "Epoch [3/10], Loss: 0.4125, Accuracy: 87.69%\n",
      "Epoch [4/10], Loss: 0.3863, Accuracy: 87.69%\n",
      "Epoch [5/10], Loss: 0.3639, Accuracy: 87.69%\n",
      "Epoch [6/10], Loss: 0.3498, Accuracy: 87.69%\n",
      "Epoch [7/10], Loss: 0.3426, Accuracy: 87.69%\n",
      "Epoch [8/10], Loss: 0.3364, Accuracy: 87.69%\n",
      "Epoch [9/10], Loss: 0.3241, Accuracy: 87.69%\n",
      "Epoch [10/10], Loss: 0.3053, Accuracy: 87.69%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Compute loss\n",
    "    \n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "    correct = (predictions == y_train).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_train.size(0)  # Compute accuracy percentage\n",
    "    \n",
    "    # Backward pass & optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1: 8609.27%\n",
      "Accuracy 2: 8609.27%\n",
      "Accuracy 3: 8609.27%\n",
      "Accuracy 4: 8609.27%\n",
      "Accuracy 5: 8609.27%\n",
      "Accuracy 6: 8609.27%\n",
      "Accuracy 7: 8609.27%\n",
      "Accuracy 8: 8609.27%\n",
      "Accuracy 9: 8609.27%\n",
      "Accuracy 10: 8609.27%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = (predictions == y_test).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_test.size(0) * 100  # Compute accuracy percentage\n",
    "    print(f\"Accuracy {epoch+1}: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Different Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'tensorflow'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
    "\n",
    "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
    "pd_all['Title+Body'] = pd_all.apply(\n",
    "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
    "pd_tplusb = pd_all.rename(columns={\n",
    "    \"Unnamed: 0\": \"id\",\n",
    "    \"class\": \"sentiment\",\n",
    "    \"Title+Body\": \"text\"\n",
    "})\n",
    "pd_tplusb.to_csv('Title+BodyCaffe.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
    "\n",
    "########## 4. Configure parameters & Start training ##########\n",
    "\n",
    "# ========== Key Configurations ==========\n",
    "\n",
    "# 1) Data file to read\n",
    "datafile = 'Title+BodyCaffe.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "out_csv_name = f'../{project}_NB.csv'\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "\n",
    "# ========== Hyperparameter grid ==========\n",
    "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}\n",
    "\n",
    "# Lists to store metrics across repeated runs\n",
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data[text_col], data['sentiment'], train_size=0.8, random_state=42)\n",
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_new = tfidf.fit_transform(data[text_col])\n",
    "X_new = torch.tensor(X_new.toarray(), dtype=torch.float32)\n",
    "y_new= torch.tensor(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1: 8127.52%\n",
      "Accuracy 2: 8127.52%\n",
      "Accuracy 3: 8127.52%\n",
      "Accuracy 4: 8127.52%\n",
      "Accuracy 5: 8127.52%\n",
      "Accuracy 6: 8127.52%\n",
      "Accuracy 7: 8127.52%\n",
      "Accuracy 8: 8127.52%\n",
      "Accuracy 9: 8127.52%\n",
      "Accuracy 10: 8127.52%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_new)\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Get predicted class (0 or 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = (predictions == y_new).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / y_new.size(0) * 100  # Compute accuracy percentage\n",
    "    print(f\"Accuracy {epoch+1}: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
