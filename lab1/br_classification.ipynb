{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Text and feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Evaluation and tuning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_curve, auc)\n",
    "\n",
    "# Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Text cleaning & stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dwika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes + TF-IDF Results ===\n",
      "Number of repeats:     10\n",
      "Average Accuracy:      0.6238\n",
      "Average Precision:     0.6056\n",
      "Average Recall:        0.7402\n",
      "Average F1 score:      0.5519\n",
      "Average AUC:           0.7402\n",
      "\n",
      "Results have been saved to: ../pytorch_NB.csv\n"
     ]
    }
   ],
   "source": [
    "from br_classification import remove_html,remove_emoji,remove_stopwords,clean_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_stop_words_list = stopwords.words('english')\n",
    "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
    "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
    "\n",
    "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
    "pd_all['Title+Body'] = pd_all.apply(\n",
    "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
    "pd_tplusb = pd_all.rename(columns={\n",
    "    \"Unnamed: 0\": \"id\",\n",
    "    \"class\": \"sentiment\",\n",
    "    \"Title+Body\": \"text\"\n",
    "})\n",
    "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'pytorch'\n",
    "path = f'datasets/{project}.csv'\n",
    "\n",
    "pd_all = pd.read_csv(path)\n",
    "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 4. Configure parameters & Start training ##########\n",
    "\n",
    "# ========== Key Configurations ==========\n",
    "\n",
    "# 1) Data file to read\n",
    "datafile = 'Title+Body.csv'\n",
    "\n",
    "# 2) Number of repeated experiments\n",
    "REPEAT = 10\n",
    "\n",
    "# 3) Output CSV file name\n",
    "out_csv_name = f'../{project}_NB.csv'\n",
    "\n",
    "# ========== Read and clean data ==========\n",
    "data = pd.read_csv(datafile).fillna('')\n",
    "text_col = 'text'\n",
    "\n",
    "# Keep a copy for referencing original data if needed\n",
    "original_data = data.copy()\n",
    "\n",
    "# Text cleaning\n",
    "data[text_col] = data[text_col].apply(remove_html)\n",
    "data[text_col] = data[text_col].apply(remove_emoji)\n",
    "data[text_col] = data[text_col].apply(remove_stopwords)\n",
    "data[text_col] = data[text_col].apply(clean_str)\n",
    "\n",
    "# ========== Hyperparameter grid ==========\n",
    "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
    "params = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 13)\n",
    "}\n",
    "\n",
    "# Lists to store metrics across repeated runs\n",
    "accuracies  = []\n",
    "precisions  = []\n",
    "recalls     = []\n",
    "f1_scores   = []\n",
    "auc_values  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Number</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>688</td>\n",
       "      <td>17355</td>\n",
       "      <td>0</td>\n",
       "      <td>the python doc global op carried source. bug m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>616</td>\n",
       "      <td>13787</td>\n",
       "      <td>0</td>\n",
       "      <td>dataloader segmentation fault using mpi backen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>322</td>\n",
       "      <td>18998</td>\n",
       "      <td>0</td>\n",
       "      <td>torch.from pil( ) request ? . feature a simple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241</td>\n",
       "      <td>4302</td>\n",
       "      <td>0</td>\n",
       "      <td>torch.halftensor object attribute mean. result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>647</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>support dilation conv1d conv3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>225</td>\n",
       "      <td>14864</td>\n",
       "      <td>0</td>\n",
       "      <td>discussion recommend different file extension ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>712</td>\n",
       "      <td>24991</td>\n",
       "      <td>0</td>\n",
       "      <td>feature request add support selu activation ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>481</td>\n",
       "      <td>14653</td>\n",
       "      <td>0</td>\n",
       "      <td>jit error reporting imported modules highlight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>348</td>\n",
       "      <td>19969</td>\n",
       "      <td>0</td>\n",
       "      <td>libtorch segmentation fault rhel 7 easy reprod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>448</td>\n",
       "      <td>29117</td>\n",
       "      <td>0</td>\n",
       "      <td>tracking issue rpc tests flaky. cc ezyang gcha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Number  sentiment                                               text\n",
       "0    688   17355          0  the python doc global op carried source. bug m...\n",
       "1    616   13787          0  dataloader segmentation fault using mpi backen...\n",
       "2    322   18998          0  torch.from pil( ) request ? . feature a simple...\n",
       "3    241    4302          0  torch.halftensor object attribute mean. result...\n",
       "4    647     367          0                     support dilation conv1d conv3d\n",
       "..   ...     ...        ...                                                ...\n",
       "747  225   14864          0  discussion recommend different file extension ...\n",
       "748  712   24991          0  feature request add support selu activation ca...\n",
       "749  481   14653          0  jit error reporting imported modules highlight...\n",
       "750  348   19969          0  libtorch segmentation fault rhel 7 easy reprod...\n",
       "751  448   29117          0  tracking issue rpc tests flaky. cc ezyang gcha...\n",
       "\n",
       "[752 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([495, 251,  97, 520, 473, 401, 618, 181, 586, 261,  17, 666, 252,\n",
       "       333, 334, 293, 691, 386, 678, 465,  45, 312,  62, 712, 327, 267,\n",
       "       140, 302, 533, 127, 424, 364, 625, 144, 355,  21, 597, 392,  35,\n",
       "       456, 240, 299,  77, 746, 241, 272, 109, 242, 417, 101, 306, 258,\n",
       "       230, 576, 285, 735, 395, 356, 741, 698, 211, 253, 165, 188, 578,\n",
       "       268,  34, 545, 316, 748, 249, 460, 624, 155, 271, 427, 468, 517,\n",
       "       200, 247, 729, 319,  12, 161, 413, 453, 493, 692, 518, 159, 529,\n",
       "       428, 229,  78,  92, 523,  66, 303, 352, 609, 310, 687, 369, 409,\n",
       "       580, 667,  15, 245, 283,   6, 313, 331, 104, 436, 390, 416,  90,\n",
       "       725, 389, 452, 218, 570, 530, 205, 738, 190, 477, 367, 194, 467,\n",
       "       132, 233, 173, 178, 727, 569, 206, 536,  96, 645, 587, 425,  89,\n",
       "       553, 179,   0,  46, 171, 742, 362, 107, 133, 496, 223, 582, 102,\n",
       "       108, 213, 740, 315, 246, 602, 407, 125, 437, 583, 399, 608, 224,\n",
       "        26, 623, 527, 492,   3, 134, 325, 350, 186, 564, 295, 749, 504,\n",
       "       593, 676, 225, 616, 330, 412, 332, 358, 511, 726, 717, 489, 498,\n",
       "       380, 566, 112, 483, 661,  20,  65, 298, 126, 259, 750, 482, 339,\n",
       "       630, 137, 484,   7, 342, 689, 309, 522, 185, 513, 353, 443, 153,\n",
       "        54,  30, 640, 669, 100, 734, 619, 711, 237, 653,  56,  60, 485,\n",
       "       262, 683, 264, 699, 441, 208, 458, 432, 167,  38, 721, 648, 615,\n",
       "       300, 491, 478, 638, 163, 124, 154, 336,  59, 476, 304, 703, 590,\n",
       "       343, 419, 311, 457, 158,  51, 540, 671, 500, 361, 621, 471, 219,\n",
       "       308,  74, 282, 499, 524,   4, 604, 668, 481, 662,   5, 141, 548,\n",
       "       135, 497, 378, 672,  22, 679, 276, 284, 270, 426, 718, 281, 420,\n",
       "       220, 680, 320, 439, 372, 120,  81, 737,  13, 596, 501, 728, 720,\n",
       "       160, 733, 238, 379, 494, 601, 440, 535, 561, 195, 191, 116, 646,\n",
       "       716, 164, 106,  16,  63, 470, 384, 695, 105, 532, 706, 329, 475,\n",
       "       480, 345, 656, 405, 670,  93, 626, 433,  83, 635, 348, 584, 455,\n",
       "       198, 145, 414, 150, 558,  39, 549, 557, 686, 579, 322, 357,  69,\n",
       "       514, 509, 340, 221, 700, 146, 289,  29, 114, 508, 663, 176, 168,\n",
       "       347, 376, 571, 657, 642, 577, 189, 136, 446, 632, 254, 664, 290,\n",
       "       708, 637, 232,  33,  88,  44, 341,  61, 719, 199, 429, 563, 394,\n",
       "       297,  73, 393, 547, 541, 643, 627, 682, 217, 539, 622, 421, 138,\n",
       "       212, 634, 631, 652, 690, 234,  67,  24, 381, 216, 129, 349, 111,\n",
       "       166, 207, 438, 552, 274, 730, 731, 525, 287, 469, 326, 121, 507,\n",
       "       228, 445, 117, 464,  25, 110, 149, 152, 528, 461, 598, 139, 260,\n",
       "       323, 710, 248, 450, 410, 607,  19, 328, 296, 269, 226,  94, 515,\n",
       "       280, 286, 589, 701, 444, 184, 371, 658, 732, 275, 704, 182,  32,\n",
       "        80, 307,  11,  43,  86,  36,  58,  41, 411, 562, 209, 148, 594,\n",
       "       123, 574,  98, 377, 130,  23, 655, 555, 370, 512, 383, 201, 368,\n",
       "       554, 610, 387, 292, 256, 606, 197,  95, 724, 169, 581, 305, 560,\n",
       "       373, 227, 660, 143, 180, 131, 715,  47, 324, 203,  84, 633, 565,\n",
       "       611, 398,  91,  82, 430, 119, 291,  57, 321, 257, 713, 442,  42,\n",
       "       617, 388, 335, 273, 488, 550,  53, 736, 128,  28, 183, 459, 510,\n",
       "       675, 151, 244, 543, 544, 639, 697, 265, 288, 423, 147, 659, 177,\n",
       "        99, 448, 431, 709, 115,  72, 537, 677, 739, 174,  87, 551, 486,\n",
       "       705, 314, 396, 600, 472,  70, 599, 277, 723,   9, 359, 707, 192,\n",
       "       629, 559, 684])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "train_index, test_index = train_test_split(\n",
    "        indices, test_size=0.2, random_state=0\n",
    "    )\n",
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([647, 142, 415,  79, 214,  40, 397,  27, 263, 318, 673, 317, 422,\n",
       "        14, 572, 531,  31, 534, 651, 479, 162, 406, 344, 463, 418, 391,\n",
       "       449, 193, 346, 156, 202, 538, 722, 278, 103, 516,   8, 243, 505,\n",
       "       122, 747, 613, 568,  75, 204, 404, 157, 222,   1, 294, 434,  55,\n",
       "       650, 382, 693, 665, 363, 365, 585, 235, 542, 113, 170, 490, 636,\n",
       "       210, 674, 612, 556, 605, 337, 521, 614, 454, 567, 546, 573, 375,\n",
       "       250,  85, 236, 487, 187,  18, 172, 688, 595, 702,  50, 744, 519,\n",
       "       714, 408, 592, 385, 374,  10,  68, 279, 255,   2, 745, 451, 743,\n",
       "       654, 435, 644, 588, 696, 231, 351, 239, 400, 681, 175, 215, 366,\n",
       "       360, 591, 526,  71,  49, 266, 402, 354, 649, 502, 474,  37, 694,\n",
       "        48, 685, 628, 196, 503,  76,  64, 603,  52, 301, 403, 506, 338,\n",
       "       641, 620, 462, 466, 118, 447, 575, 751])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495    getting build error. i building pytorch i gett...\n",
       "251    document torch.quantize per tensor torch.quant...\n",
       "97     bug fail throw error computing loss tensors sh...\n",
       "520    feature request pytorch more flexible optimize...\n",
       "473    nccl hang pytorch distributed data parallel mi...\n",
       "                             ...                        \n",
       "707    mysterious tensor indexing problem. bug indexi...\n",
       "192    migrate ` multinomial alias setup` th aten (cp...\n",
       "629    build nccl failed build libnccl debian unstabl...\n",
       "559    switch cuda svd qr using cusolver. currently u...\n",
       "684    detach working properly stochastic variables. ...\n",
       "Name: text, Length: 601, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = data[text_col].iloc[train_index]\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data['sentiment'].iloc[train_index]\n",
    "y_test  = data['sentiment'].iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = data[text_col].iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_train = tfidf.fit_transform(train_text) # this one traisn\n",
    "X_test = tfidf.transform(test_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes + TF-IDF Results ===\n",
      "Number of repeats:     10\n",
      "Average Accuracy:      0.6238\n",
      "Average Precision:     0.6056\n",
      "Average Recall:        0.7402\n",
      "Average F1 score:      0.5519\n",
      "Average AUC:           0.7402\n",
      "\n",
      "Results have been saved to: ../pytorch_NB.csv\n"
     ]
    }
   ],
   "source": [
    "for repeated_time in range(REPEAT):\n",
    "    # --- 4.1 Split into train/test ---\n",
    "    indices = np.arange(data.shape[0])\n",
    "    train_index, test_index = train_test_split(\n",
    "        indices, test_size=0.2, random_state=repeated_time\n",
    "    )\n",
    "\n",
    "    train_text = data[text_col].iloc[train_index]\n",
    "    test_text = data[text_col].iloc[test_index]\n",
    "\n",
    "    y_train = data['sentiment'].iloc[train_index]\n",
    "    y_test  = data['sentiment'].iloc[test_index]\n",
    "\n",
    "    # --- 4.2 TF-IDF vectorization ---\n",
    "    tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "    X_train = tfidf.fit_transform(train_text)\n",
    "    X_test = tfidf.transform(test_text)\n",
    "   \n",
    "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
    "    clf = GaussianNB()\n",
    "    grid = GridSearchCV(\n",
    "        clf,\n",
    "        params,\n",
    "        cv=5,              # 5-fold CV (can be changed)\n",
    "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
    "    )\n",
    "    grid.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    # Retrieve the best model\n",
    "    best_clf = grid.best_estimator_\n",
    "    best_clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    # --- 4.4 Make predictions & evaluate ---\n",
    "    y_pred = best_clf.predict(X_test.toarray())\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    # Precision (macro)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    precisions.append(prec)\n",
    "\n",
    "    # Recall (macro)\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    recalls.append(rec)\n",
    "\n",
    "    # F1 Score (macro)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # AUC\n",
    "    # If labels are 0/1 only, this works directly.\n",
    "    # If labels are something else, adjust pos_label accordingly.\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    auc_values.append(auc_val)\n",
    "\n",
    "# --- 4.5 Aggregate results ---\n",
    "final_accuracy  = np.mean(accuracies)\n",
    "final_precision = np.mean(precisions)\n",
    "final_recall    = np.mean(recalls)\n",
    "final_f1        = np.mean(f1_scores)\n",
    "final_auc       = np.mean(auc_values)\n",
    "\n",
    "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
    "print(f\"Number of repeats:     {REPEAT}\")\n",
    "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
    "print(f\"Average Precision:     {final_precision:.4f}\")\n",
    "print(f\"Average Recall:        {final_recall:.4f}\")\n",
    "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
    "print(f\"Average AUC:           {final_auc:.4f}\")\n",
    "\n",
    "# Save final results to CSV (append mode)\n",
    "try:\n",
    "    # Attempt to check if the file already has a header\n",
    "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
    "    header_needed = False\n",
    "except:\n",
    "    header_needed = True\n",
    "\n",
    "df_log = pd.DataFrame(\n",
    "    {\n",
    "        'repeated_times': [REPEAT],\n",
    "        'Accuracy': [final_accuracy],\n",
    "        'Precision': [final_precision],\n",
    "        'Recall': [final_recall],\n",
    "        'F1': [final_f1],\n",
    "        'AUC': [final_auc],\n",
    "        'CV_list(AUC)': [str(auc_values)]\n",
    "    }\n",
    ")\n",
    "\n",
    "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
    "\n",
    "print(f\"\\nResults have been saved to: {out_csv_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = data[text_col].iloc[train_index]\n",
    "test_text = data[text_col].iloc[test_index]\n",
    "\n",
    "y_train = data['sentiment'].iloc[train_index]\n",
    "y_test  = data['sentiment'].iloc[test_index]\n",
    "\n",
    "    # --- 4.2 TF-IDF vectorization ---\n",
    "tfidf = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=1000  # Adjust as needed\n",
    "    )\n",
    "X_train = tfidf.fit_transform(train_text)\n",
    "X_test = tfidf.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<601x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 26895 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fyp)",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
